# bayesian-xgboost-project
Bayesian Optimization for XGBoost Regression
[untitled37.py](https://github.com/user-attachments/files/23784412/untitled37.py)
# -*- coding: utf-8 -*-
"""Untitled37.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ec362St0K5HaUXjt7168w7NOjDnoIbPx
"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.datasets import fetch_california_housing
from xgboost import XGBRegressor
from scipy.stats import norm
import warnings
warnings.filterwarnings("ignore")

# 1. Load Dataset

data = fetch_california_housing()
X = data.data
y = data.target

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

# 2. Define Hyperparameter Search Space

def sample_random_params():
    return {
        "learning_rate": np.random.uniform(0.01, 0.3),
        "subsample": np.random.uniform(0.5, 1.0),
        "colsample_bytree": np.random.uniform(0.5, 1.0),
        "gamma": np.random.uniform(0, 5),
        "max_depth": np.random.randint(3, 12),
        "tree_method": np.random.choice(["auto", "exact"]) }

def params_to_vector(p):
    tree_map = {"auto": 0, "exact": 1}
    return np.array([
        p["learning_rate"],
        p["subsample"],
        p["colsample_bytree"],
        p["gamma"],
        p["max_depth"],
        tree_map[p["tree_method"]]])

# 3. Objective Function

def objective(params):
    model = XGBRegressor(
        learning_rate=params["learning_rate"],
        subsample=params["subsample"],
        colsample_bytree=params["colsample_bytree"],
        gamma=params["gamma"],
        max_depth=params["max_depth"],
        tree_method=params["tree_method"],
        n_estimators=200,
        random_state=42
    )
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    return mean_squared_error(y_test, preds)

# 4. Expected Improvement

def expected_improvement(X, X_sample, Y_sample, gpr):
    mu, sigma = gpr.predict(X, return_std=True)
    sigma = sigma.reshape(-1, 1)

    Y_best = np.min(Y_sample)

    with np.errstate(divide="ignore"):
        Z = (Y_best - mu).reshape(-1, 1) / sigma
        ei = ((Y_best - mu).reshape(-1, 1)) * norm.cdf(Z) + sigma * norm.pdf(Z)
        ei[sigma == 0.0] = 0

    return ei

# 5. Bayesian Optimization Loop

X_sample = []
Y_sample = []

for _ in range(5):
    p = sample_random_params()
    X_sample.append(params_to_vector(p))
    Y_sample.append(objective(p))

X_sample = np.array(X_sample)
Y_sample = np.array(Y_sample)

kernel = C(1.0) * RBF(length_scale=1.0)

for iteration in range(50):
    gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True)
    gpr.fit(X_sample, Y_sample)

    candidates = np.array([
        params_to_vector(sample_random_params())
        for _ in range(200)])

    ei = expected_improvement(candidates, X_sample, Y_sample, gpr)
    best_index = np.argmax(ei)
    next_point = candidates[best_index]

    def decode_vector(v):
        tree_method = "auto" if v[5] < 0.5 else "exact"
        return {
            "learning_rate": float(v[0]),
            "subsample": float(v[1]),
            "colsample_bytree": float(v[2]),
            "gamma": float(v[3]),
            "max_depth": int(v[4]),
            "tree_method": tree_method }

    next_params = decode_vector(next_point)

    mse = objective(next_params)

    X_sample = np.vstack((X_sample, next_point))
    Y_sample = np.append(Y_sample, mse)

    print(f"Iteration {iteration+1} - MSE = {mse:.4f}")

# 6. Best Results
best_idx = np.argmin(Y_sample)
best_params = decode_vector(X_sample[best_idx])

print("\nBest Hyperparameters Found:")
print(best_params)

print("\nBest MSE:", Y_sample[best_idx])
